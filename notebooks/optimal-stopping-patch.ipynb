{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5702ea7",
   "metadata": {},
   "source": [
    "# Optimal stopping notebook patch (from `optimal-stopping.json`)\n",
    "\n",
    "This notebook **contains the unified diff/patch** that updates `model-training + Behav Cloning.ipynb` (adds low-RAM defaults, additional comments, and a few safer training/generation settings).\n",
    "\n",
    "## How to use in your GitHub repo\n",
    "\n",
    "1. Put this notebook in your repo under `notebooks/` (or anywhere).\n",
    "2. Run the first code cell to write the patch to `optimal-stopping.patch`.\n",
    "3. From your repo root, apply it:\n",
    "\n",
    "```bash\n",
    "git apply notebooks/optimal-stopping.patch\n",
    "```\n",
    "\n",
    "If the target notebook path in your repo is different, edit the patch headers accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "## Patch content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writes the patch to disk so you can apply it with `git apply`.\n",
    "from pathlib import Path\n",
    "\n",
    "patch_path = Path(\"optimal-stopping.patch\")\n",
    "patch_path.write_text(r'''diff --git a/notebooks/model-training + Behav Cloning.ipynb b/notebooks/model-training + Behav Cloning.ipynb\n",
    "index ac4b69a5e9d865f3f2baa48862ee346b15160954..1a36d43c67f162e91a1ece9314db97c1f914989d 100644\n",
    "--- a/notebooks/model-training + Behav Cloning.ipynb\t\n",
    "+++ b/notebooks/model-training + Behav Cloning.ipynb\t\n",
    "@@ -17,69 +17,80 @@\n",
    "    \"source\": [\n",
    "     \"## Imports\\n\",\n",
    "     \"\\n\",\n",
    "     \"Before running this notebook locally, you need to [install PyTorch](https://pytorch.org/get-started/locally/) for your hardware.\\n\",\n",
    "     \"\\n\",\n",
    "     \"Then, you need to install the following packages:\\n\",\n",
    "     \"\\n\",\n",
    "     \"   * transformers\\n\",\n",
    "     \"   * datasets\\n\",\n",
    "     \"   * accelerate\\n\",\n",
    "     \"   * pandas\\n\",\n",
    "     \"   * huggingface_hub (needed for Llama models)\\n\",\n",
    "     \"   * scikit-learn\\n\",\n",
    "     \"   * numpy\\n\",\n",
    "     \"\\n\",\n",
    "     \"You an also use the `requirements.txt` in the [stopping-agents](https://github.com/emaadmanzoor/stopping-agents) repository.\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"code\",\n",
    "    \"execution_count\": null,\n",
    "    \"id\": \"cf5fb535\",\n",
    "    \"metadata\": {},\n",
    "    \"outputs\": [],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Imports libraries and defines global experiment/config constants used across the notebook.\\n\",\n",
    "+    \"# Why we choose this setup: Keeping core knobs in one place makes Colab/laptop tuning explicit and reduces trial-and-error when memory is limited.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"import datasets\\n\",\n",
    "+    \"import gc\\n\",\n",
    "     \"import huggingface_hub # needed for Llama models\\n\",\n",
    "     \"import math\\n\",\n",
    "     \"import numpy as np\\n\",\n",
    "     \"import pandas as pd\\n\",\n",
    "     \"import torch\\n\",\n",
    "     \"import transformers\\n\",\n",
    "     \"\\n\",\n",
    "     \"from sklearn.model_selection import train_test_split\\n\",\n",
    "     \"from sklearn.metrics import roc_auc_score\\n\",\n",
    "     \"from tqdm.auto import tqdm\\n\",\n",
    "     \"\\n\",\n",
    "     \"HF_TOKEN = \\\"HF_TOKEN\\\"\\n\",\n",
    "     \"\\n\",\n",
    "     \"COST_PER_UNIT_TIME = 0.1\\n\",\n",
    "     \"BENEFIT_PER_POSITIVE_OUTCOME = 10.0\\n\",\n",
    "     \"DECISION_OPPORTUNITIES = [45, 60] # time in seconds at which the \\n\",\n",
    "     \"                                  # agent can decide to quit or wait\\n\",\n",
    "-    \"                                  # code is tailored to just 2 right now\"\n",
    "+    \"                                  # code is tailored to just 2 right now\\n\",\n",
    "+    \"\\n\",\n",
    "+    \"LOW_RAM_MODE = True  # Recommended for Colab/local laptops\\n\",\n",
    "+    \"MAX_SEQ_LEN = 768 if LOW_RAM_MODE else 1024\\n\",\n",
    "+    \"TRAIN_BATCH_SIZE = 1 if LOW_RAM_MODE else 12\\n\",\n",
    "+    \"EVAL_BATCH_SIZE = 1 if LOW_RAM_MODE else 12\\n\",\n",
    "+    \"GRAD_ACCUM_STEPS = 16 if LOW_RAM_MODE else 1\\n\",\n",
    "+    \"GEN_BATCH_SIZE = 16 if LOW_RAM_MODE else 72\\n\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"markdown\",\n",
    "    \"id\": \"931864dc\",\n",
    "    \"metadata\": {},\n",
    "    \"source\": [\n",
    "     \"## Load and process conversation data\\n\",\n",
    "     \"\\n\",\n",
    "     \"We load a dataset of synthetic conversations available\\n\",\n",
    "     \"in the `datasets` folder at [https://github.com/emaadmanzoor/stopping-agents/](https://github.com/emaadmanzoor/stopping-agents/). This example dataset is formatted in the PyAnnote diarized conversation format.\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"code\",\n",
    "    \"execution_count\": 2,\n",
    "    \"id\": \"99c5e357\",\n",
    "    \"metadata\": {},\n",
    "    \"outputs\": [\n",
    "     {\n",
    "      \"data\": {\n",
    "       \"text/html\": [\n",
    "        \"<div>\\n\",\n",
    "        \"<style scoped>\\n\",\n",
    "        \"    .dataframe tbody tr th:only-of-type {\\n\",\n",
    "@@ -175,89 +186,95 @@\n",
    "        \"2         20756_1           0        7.98     12.84   \\n\",\n",
    "        \"3         20756_1           1       13.14     15.50   \\n\",\n",
    "        \"4         20756_1           0       15.89     22.14   \\n\",\n",
    "        \"\\n\",\n",
    "        \"                                                text  outcome  is_sale  \\\\\\n\",\n",
    "        \"0  Hello, is this Mr. Harris? My name is Leah fro...  no sale        0   \\n\",\n",
    "        \"1  Yes, speaking. I’m alright, thanks. Can I ask ...  no sale        0   \\n\",\n",
    "        \"2  Of course, thanks for asking. I’m reaching out...  no sale        0   \\n\",\n",
    "        \"3        Alright… I guess I can listen for a minute.  no sale        0   \\n\",\n",
    "        \"4  Thank you! So, our new BrightSaver plan locks ...  no sale        0   \\n\",\n",
    "        \"\\n\",\n",
    "        \"   duration  \\n\",\n",
    "        \"0     62.07  \\n\",\n",
    "        \"1     62.07  \\n\",\n",
    "        \"2     62.07  \\n\",\n",
    "        \"3     62.07  \\n\",\n",
    "        \"4     62.07  \"\n",
    "       ]\n",
    "      },\n",
    "      \"execution_count\": 2,\n",
    "      \"metadata\": {},\n",
    "      \"output_type\": \"execute_result\"\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Downloads the synthetic sales-call dataset and creates target/metadata columns.\\n\",\n",
    "+    \"# Why we choose this setup: We derive `is_sale` and call `duration` once up front so every later stage (splits, rewards, training labels) uses consistent ground truth.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"dataset_url = \\\"https://raw.githubusercontent.com/emaadmanzoor/stopping-agents/refs/heads/main/datasets/synthetic_sales_conversations.csv?token=GHSAT0AAAAAADBUAD4WOA6XRF2GSIX5UC4Y2EEF66Q\\\"\\n\",\n",
    "     \"\\n\",\n",
    "     \"diarized_conversations = pd.read_csv(dataset_url)\\n\",\n",
    "     \"\\n\",\n",
    "     \"diarized_conversations[\\\"is_sale\\\"] =\\\\\\n\",\n",
    "     \"        diarized_conversations[\\\"outcome\\\"].apply(\\n\",\n",
    "     \"            lambda x: 1 if x == \\\"sale\\\" else 0 if x == \\\"no sale\\\" else np.nan)\\n\",\n",
    "     \"\\n\",\n",
    "     \"diarized_conversations[\\\"duration\\\"] =\\\\\\n\",\n",
    "     \"    diarized_conversations.groupby(\\\"conversation_id\\\")[\\\"end_time\\\"].transform(\\\"max\\\")\\n\",\n",
    "     \"\\n\",\n",
    "     \"diarized_conversations.head()\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"markdown\",\n",
    "    \"id\": \"cefd33a2\",\n",
    "    \"metadata\": {},\n",
    "    \"source\": [\n",
    "     \"### Split into train, validation, and test conversations\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"code\",\n",
    "    \"execution_count\": 3,\n",
    "    \"id\": \"a51e8881\",\n",
    "    \"metadata\": {},\n",
    "    \"outputs\": [\n",
    "     {\n",
    "      \"name\": \"stdout\",\n",
    "      \"output_type\": \"stream\",\n",
    "      \"text\": [\n",
    "       \"1903 train conversations.\\n\",\n",
    "       \"651 validation conversations.\\n\",\n",
    "       \"860 test conversations.\\n\"\n",
    "      ]\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Splits conversations into train/validation/test sets with label stratification.\\n\",\n",
    "+    \"# Why we choose this setup: Splitting by conversation_id avoids leakage across turns from the same call, and stratification preserves class balance in each split.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"all_conversation_ids =\\\\\\n\",\n",
    "     \"    diarized_conversations[[\\\"conversation_id\\\", \\\"is_sale\\\"]].drop_duplicates()[\\\"conversation_id\\\"]\\\\\\n\",\n",
    "     \"        .values\\n\",\n",
    "     \"all_outcomes =\\\\\\n\",\n",
    "     \"    diarized_conversations[[\\\"conversation_id\\\", \\\"is_sale\\\"]].drop_duplicates()[\\\"is_sale\\\"].values\\n\",\n",
    "     \"    \\n\",\n",
    "     \"train_conversation_ids, test_conversation_ids, train_outcomes, test_outcomes =\\\\\\n\",\n",
    "     \"    train_test_split(all_conversation_ids, all_outcomes, test_size=0.25, random_state=42,\\n\",\n",
    "     \"                     stratify=all_outcomes)\\n\",\n",
    "     \"train_conversation_ids, val_conversation_ids, train_outcomes, val_outcomes =\\\\\\n\",\n",
    "     \"    train_test_split(train_conversation_ids, train_outcomes, test_size=0.25, random_state=42,\\n\",\n",
    "     \"                     stratify=train_outcomes)\\n\",\n",
    "     \"\\n\",\n",
    "     \"diarized_conversations_train =\\\\\\n\",\n",
    "     \"    diarized_conversations[diarized_conversations[\\\"conversation_id\\\"].isin(train_conversation_ids)]\\n\",\n",
    "     \"diarized_conversations_val =\\\\\\n\",\n",
    "     \"    diarized_conversations[diarized_conversations[\\\"conversation_id\\\"].isin(val_conversation_ids)]\\n\",\n",
    "     \"diarized_conversations_test =\\\\\\n\",\n",
    "     \"    diarized_conversations[diarized_conversations[\\\"conversation_id\\\"].isin(test_conversation_ids)]\\n\",\n",
    "     \"\\n\",\n",
    "     \"print(len(diarized_conversations_train), \\\"train conversations.\\\")\\n\",\n",
    "     \"print(len(diarized_conversations_val), \\\"validation conversations.\\\")\\n\",\n",
    "     \"print(len(diarized_conversations_test), \\\"test conversations.\\\")\"\n",
    "    ]\n",
    "   },\n",
    "@@ -364,50 +381,53 @@\n",
    "        \"2         92837_7     43.64        0   \\n\",\n",
    "        \"3         58241_9     50.01        0   \\n\",\n",
    "        \"4        20567_11     45.19        0   \\n\",\n",
    "        \"\\n\",\n",
    "        \"                               transcript_speaker_45  \\\\\\n\",\n",
    "        \"0  Speaker 0: Hello, is this Mr. Harris? My name ...   \\n\",\n",
    "        \"1  Speaker 0: Good afternoon! Is this Ms. Parker?...   \\n\",\n",
    "        \"2  Speaker 0: Good afternoon, may I speak with Mr...   \\n\",\n",
    "        \"3  Speaker 0: Hello, may I speak with Ms. Jenkins...   \\n\",\n",
    "        \"4  Speaker 0: Good afternoon, is this Mr. Carver?...   \\n\",\n",
    "        \"\\n\",\n",
    "        \"                               transcript_speaker_60  \\n\",\n",
    "        \"0  Speaker 0: Hello, is this Mr. Harris? My name ...  \\n\",\n",
    "        \"1  Speaker 0: Good afternoon! Is this Ms. Parker?...  \\n\",\n",
    "        \"2  Speaker 0: Good afternoon, may I speak with Mr...  \\n\",\n",
    "        \"3  Speaker 0: Hello, may I speak with Ms. Jenkins...  \\n\",\n",
    "        \"4  Speaker 0: Good afternoon, is this Mr. Carver?...  \"\n",
    "       ]\n",
    "      },\n",
    "      \"execution_count\": 4,\n",
    "      \"metadata\": {},\n",
    "      \"output_type\": \"execute_result\"\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Builds per-conversation transcripts truncated at each decision time (m1, m2).\\n\",\n",
    "+    \"# Why we choose this setup: Behavioral cloning needs state snapshots at decision times, so we materialize both views once for reproducible feature creation.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"m1, m2 = sorted(DECISION_OPPORTUNITIES)\\n\",\n",
    "     \"\\n\",\n",
    "     \"data_transcripts = {}\\n\",\n",
    "     \"for df, dftype in zip([diarized_conversations_train,\\n\",\n",
    "     \"                       diarized_conversations_val,\\n\",\n",
    "     \"                       diarized_conversations_test],\\n\",\n",
    "     \"                      [\\\"train\\\", \\\"val\\\", \\\"test\\\"]):\\n\",\n",
    "     \"    \\n\",\n",
    "     \"    data_transcripts[dftype] = df.copy()\\n\",\n",
    "     \"\\n\",\n",
    "     \"    data_transcripts[dftype][\\\"transcript\\\"] =\\\\\\n\",\n",
    "     \"        \\\"Speaker \\\" +\\\\\\n\",\n",
    "     \"        data_transcripts[dftype][\\\"speaker_id\\\"].astype(str) + \\\": \\\" +\\\\\\n\",\n",
    "     \"        data_transcripts[dftype][\\\"text\\\"]\\n\",\n",
    "     \"\\n\",\n",
    "     \"    transcripts = {}\\n\",\n",
    "     \"    for m in [m1, m2]: \\n\",\n",
    "     \"        transcripts[m] =\\\\\\n\",\n",
    "     \"            data_transcripts[dftype].loc[(data_transcripts[dftype][\\\"end_time\\\"]>=0) &\\n\",\n",
    "     \"                                         (data_transcripts[dftype][\\\"end_time\\\"]<m)]\\\\\\n\",\n",
    "     \"                    .groupby(\\\"conversation_id\\\")[\\\"transcript\\\"]\\\\\\n\",\n",
    "     \"                    .apply(lambda x: '\\\\n'.join(x))\\\\\\n\",\n",
    "     \"                    .reset_index(name=\\\"transcript_speaker_\\\" + str(m))\\n\",\n",
    "     \"\\n\",\n",
    "     \"    data_transcripts[dftype] = \\\\\\n\",\n",
    "@@ -463,50 +483,53 @@\n",
    "       \"Speaker 0: There's no early cancellation fee and no long-term contract; you can opt out any time. We just want people to enjoy lower, predictable pricing with no risk.\\n\",\n",
    "       \"Speaker 1: To be honest, I just re-upped my plan last month. I don’t like to change stuff if it’s working.\\n\",\n",
    "       \"Will this call end in a sale (respond with 'yes' or 'no'):  \\n\",\n",
    "       \"\\n\",\n",
    "       \"Example state at 60 seconds:\\n\",\n",
    "       \"Below is the first 60 seconds of the sales call between the sales agent Speaker 0 and the customer Speaker 1:\\n\",\n",
    "       \"Speaker 0: Hello, is this Mr. Harris? My name is Leah from Sunview Energy—how are you today?\\n\",\n",
    "       \"Speaker 1: Yes, speaking. I’m alright, thanks. Can I ask what this is about?\\n\",\n",
    "       \"Speaker 0: Of course, thanks for asking. I’m reaching out because we’re offering a new energy plan that could qualify you for a 15% discount on your electric bill. I wanted to see if I could quickly tell you about it.\\n\",\n",
    "       \"Speaker 1: Alright… I guess I can listen for a minute.\\n\",\n",
    "       \"Speaker 0: Thank you! So, our new BrightSaver plan locks in your rate for twelve months—there’s no change in price based on the time of day, and there are no hidden fees. And for this month, you’d also get an automatic 15% off your supply charges.\\n\",\n",
    "       \"Speaker 1: Is this something I have to switch providers for? I’m pretty happy with who I have now.\\n\",\n",
    "       \"Speaker 0: You would stay connected to your local utility for service and repairs, but Sunview would handle the billing and supply. The switch is very simple and risk-free—if you change your mind, you can cancel within 30 days.\\n\",\n",
    "       \"Speaker 1: I see. Is there a contract or any penalties?\\n\",\n",
    "       \"Speaker 0: There's no early cancellation fee and no long-term contract; you can opt out any time. We just want people to enjoy lower, predictable pricing with no risk.\\n\",\n",
    "       \"Speaker 1: To be honest, I just re-upped my plan last month. I don’t like to change stuff if it’s working.\\n\",\n",
    "       \"Speaker 0: That’s completely understandable, Mr. Harris. Do you mind if I ask how much you’re paying per kilowatt-hour, just to make sure you’re on the best deal?\\n\",\n",
    "       \"Speaker 1: Actually, I’m not sure off the top of my head. I just check that the total seems right each month.\\n\",\n",
    "       \"Speaker 0: Totally fair. If you’re interested, I could email you a side-by-side comparison of our BrightSaver plan and your last bill—no obligation, just information.\\n\",\n",
    "       \"Speaker 1: No, that’s okay. If I decide to look into it, I’ll reach out myself.\\n\",\n",
    "       \"Will this call end in a sale (respond with 'yes' or 'no'):  \\n\"\n",
    "      ]\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Converts each transcript snapshot into a language-model prompt/state string.\\n\",\n",
    "+    \"# Why we choose this setup: A consistent prompt template reduces formatting variance and focuses the model on the yes/no sale-outcome prediction task.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"def convert_to_state(transcript, t):\\n\",\n",
    "     \"    assert type(transcript) == str\\n\",\n",
    "     \"\\n\",\n",
    "     \"    state = \\\"Below is the first \\\" + str(t) +\\\\\\n\",\n",
    "     \"            \\\" seconds of the sales call between the sales agent Speaker 0 and\\\" +\\\\\\n\",\n",
    "     \"            \\\" the customer Speaker 1:\\\\n\\\" +\\\\\\n\",\n",
    "     \"            transcript + \\\"\\\\n\\\" +\\\\\\n\",\n",
    "     \"            \\\"Will this call end in a sale (respond with 'yes' or 'no'):  \\\"\\n\",\n",
    "     \"\\n\",\n",
    "     \"    return state\\n\",\n",
    "     \"\\n\",\n",
    "     \"for df in [data_transcripts[\\\"train\\\"],\\n\",\n",
    "     \"           data_transcripts[\\\"val\\\"],\\n\",\n",
    "     \"           data_transcripts[\\\"test\\\"]]:\\n\",\n",
    "     \"\\n\",\n",
    "     \"    for m in [m1, m2]:\\n\",\n",
    "     \"        df.loc[:, \\\"s\\\" + str(m)] = df.apply(lambda x:\\n\",\n",
    "     \"                                            convert_to_state(x[\\\"transcript_speaker_\\\" + str(m)],\\n\",\n",
    "     \"                                                             m), axis=1)\\n\",\n",
    "     \"\\n\",\n",
    "     \"print(\\\"Example state at\\\", m1, \\\"seconds:\\\")\\n\",\n",
    "     \"print(data_transcripts[\\\"train\\\"][\\\"s\\\" + str(m1)].values[0])\\n\",\n",
    "     \"print()\\n\",\n",
    "     \"print(\\\"Example state at\\\", m2, \\\"seconds:\\\")\\n\",\n",
    "     \"print(data_transcripts[\\\"train\\\"][\\\"s\\\" + str(m2)].values[0])\"\n",
    "@@ -620,50 +643,53 @@\n",
    "        \"</table>\\n\",\n",
    "        \"</div>\"\n",
    "       ],\n",
    "       \"text/plain\": [\n",
    "        \"  conversation_id                                              state action  \\\\\\n\",\n",
    "        \"0         20756_1  Below is the first 45 seconds of the sales cal...     no   \\n\",\n",
    "        \"1         59321_6  Below is the first 45 seconds of the sales cal...     no   \\n\",\n",
    "        \"2         58241_9  Below is the first 45 seconds of the sales cal...     no   \\n\",\n",
    "        \"3        20567_11  Below is the first 45 seconds of the sales cal...     no   \\n\",\n",
    "        \"4        10523_13  Below is the first 45 seconds of the sales cal...     no   \\n\",\n",
    "        \"\\n\",\n",
    "        \"   is_sale  duration  \\n\",\n",
    "        \"0        0     62.07  \\n\",\n",
    "        \"1        0     55.37  \\n\",\n",
    "        \"2        0     50.01  \\n\",\n",
    "        \"3        0     45.19  \\n\",\n",
    "        \"4        0     65.04  \"\n",
    "       ]\n",
    "      },\n",
    "      \"execution_count\": 6,\n",
    "      \"metadata\": {},\n",
    "      \"output_type\": \"execute_result\"\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Computes reward outcomes under each stopping policy path and derives optimal yes/no actions.\\n\",\n",
    "+    \"# Why we choose this setup: This converts the optimal stopping objective into supervised labels so we can train via behavioral cloning.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"optimal_state_action_pairs = {}\\n\",\n",
    "     \"for dftype, df in data_transcripts.items():\\n\",\n",
    "     \"    df[\\\"rq\\\" + str(m1)] = -m1 * COST_PER_UNIT_TIME # stop at 30\\n\",\n",
    "     \"\\n\",\n",
    "     \"    # continue at 30, stop at 60\\n\",\n",
    "     \"    df[\\\"rq\\\" + str(m2)] = df[\\\"is_sale\\\"].astype(int)\\\\\\n\",\n",
    "     \"                        * BENEFIT_PER_POSITIVE_OUTCOME\\\\\\n\",\n",
    "     \"                        * (df[\\\"duration\\\"]<=m2).astype(int) \\\\\\n\",\n",
    "     \"                        - df[\\\"duration\\\"].apply(lambda x: min(m2, x)) * COST_PER_UNIT_TIME\\n\",\n",
    "     \"    \\n\",\n",
    "     \"    # continue at 30, continue at 60, continue at 90 = never quit                \\n\",\n",
    "     \"    df[\\\"rc\\\" + str(m2)] = df[\\\"is_sale\\\"].astype(int) * BENEFIT_PER_POSITIVE_OUTCOME\\\\\\n\",\n",
    "     \"                        - df[\\\"duration\\\"] * COST_PER_UNIT_TIME\\n\",\n",
    "     \"\\n\",\n",
    "     \"    df[\\\"max_reward\\\"] = df[[\\\"rq\\\" + str(m1), \\\"rq\\\" + str(m2), \\\"rc\\\" + str(m2)]].max(axis=1)\\n\",\n",
    "     \"\\n\",\n",
    "     \"    # optimal to quit at 30\\n\",\n",
    "     \"    df.loc[df[\\\"max_reward\\\"]==df[\\\"rq\\\" + str(m1)], \\\"a\\\" + str(m1)] = \\\"no\\\"\\n\",\n",
    "     \"    df.loc[df[\\\"max_reward\\\"]==df[\\\"rq\\\" + str(m1)], \\\"a\\\" + str(m2)] = \\\"no\\\"\\n\",\n",
    "     \"\\n\",\n",
    "     \"    # optimal to continue at 30, stop at 60\\n\",\n",
    "     \"    df.loc[df[\\\"max_reward\\\"]==df[\\\"rq\\\" + str(m2)], \\\"a\\\"  + str(m1)] = \\\"yes\\\"\\n\",\n",
    "     \"    df.loc[df[\\\"max_reward\\\"]==df[\\\"rq\\\" + str(m2)], \\\"a\\\"  + str(m2)] = \\\"no\\\"\\n\",\n",
    "     \"\\n\",\n",
    "     \"    # optimal to continue at 30, continue at 60, continue at 90\\n\",\n",
    "@@ -718,61 +744,83 @@\n",
    "     \"### Load base model\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"code\",\n",
    "    \"execution_count\": 7,\n",
    "    \"id\": \"db06d469\",\n",
    "    \"metadata\": {},\n",
    "    \"outputs\": [\n",
    "     {\n",
    "      \"data\": {\n",
    "       \"application/vnd.jupyter.widget-view+json\": {\n",
    "        \"model_id\": \"7aa0faf14d8d43bcaea305837ff480f4\",\n",
    "        \"version_major\": 2,\n",
    "        \"version_minor\": 0\n",
    "       },\n",
    "       \"text/plain\": [\n",
    "        \"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\"\n",
    "       ]\n",
    "      },\n",
    "      \"metadata\": {},\n",
    "      \"output_type\": \"display_data\"\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Authenticates to HF, loads tokenizer/model, and applies low-memory loading paths when possible.\\n\",\n",
    "+    \"# Why we choose this setup: 1B + optional 4-bit quantization is substantially more stable on Colab/laptop hardware while preserving the same notebook workflow.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"huggingface_hub.login(token=HF_TOKEN)\\n\",\n",
    "     \"\\n\",\n",
    "-    \"model_name = \\\"meta-llama/Llama-3.2-3B\\\" # base model, not instruction-tuned\\n\",\n",
    "+    \"# 1B is much easier to run on Colab/laptops than 3B.\\n\",\n",
    "+    \"model_name = \\\"meta-llama/Llama-3.2-1B\\\" if LOW_RAM_MODE else \\\"meta-llama/Llama-3.2-3B\\\"\\n\",\n",
    "     \"tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\\n\",\n",
    "-    \"model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\\\"auto\\\")\\n\",\n",
    "     \"tokenizer.pad_token_id = tokenizer.eos_token_id\\n\",\n",
    "     \"tokenizer.padding_side = 'left'\\n\",\n",
    "     \"\\n\",\n",
    "+    \"if torch.cuda.is_available() and LOW_RAM_MODE:\\n\",\n",
    "+    \"    bnb_config = transformers.BitsAndBytesConfig(\\n\",\n",
    "+    \"        load_in_4bit=True,\\n\",\n",
    "+    \"        bnb_4bit_quant_type=\\\"nf4\\\",\\n\",\n",
    "+    \"        bnb_4bit_compute_dtype=torch.float16,\\n\",\n",
    "+    \"    )\\n\",\n",
    "+    \"    model = transformers.AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "+    \"        model_name,\\n\",\n",
    "+    \"        quantization_config=bnb_config,\\n\",\n",
    "+    \"        device_map=\\\"auto\\\",\\n\",\n",
    "+    \"        low_cpu_mem_usage=True,\\n\",\n",
    "+    \"    )\\n\",\n",
    "+    \"else:\\n\",\n",
    "+    \"    model = transformers.AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "+    \"        model_name,\\n\",\n",
    "+    \"        torch_dtype=\\\"auto\\\",\\n\",\n",
    "+    \"        low_cpu_mem_usage=True,\\n\",\n",
    "+    \"    )\\n\",\n",
    "+    \"\\n\",\n",
    "     \"if len(tokenizer) > model.get_input_embeddings().weight.shape[0]:\\n\",\n",
    "     \"    print(\\\"WARNING: Resizing the embedding matrix to match the tokenizer vocab size.\\\")\\n\",\n",
    "-    \"    model.resize_token_embeddings(len(tokenizer))\"\n",
    "+    \"    model.resize_token_embeddings(len(tokenizer))\\n\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"markdown\",\n",
    "    \"id\": \"240767e7\",\n",
    "    \"metadata\": {},\n",
    "    \"source\": [\n",
    "     \"### Construct and tokenize fine-tuning datasets\\n\",\n",
    "     \"\\n\",\n",
    "     \"We perform manual masking, so the loss is only calculated for the generated actions.\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"code\",\n",
    "    \"execution_count\": 8,\n",
    "    \"id\": \"d8c94248\",\n",
    "    \"metadata\": {},\n",
    "    \"outputs\": [\n",
    "     {\n",
    "      \"data\": {\n",
    "       \"application/vnd.jupyter.widget-view+json\": {\n",
    "        \"model_id\": \"ac592b7caaa94bc4b89d7fb9d868dacf\",\n",
    "        \"version_major\": 2,\n",
    "        \"version_minor\": 0\n",
    "       },\n",
    "@@ -799,81 +847,87 @@\n",
    "     },\n",
    "     {\n",
    "      \"name\": \"stdout\",\n",
    "      \"output_type\": \"stream\",\n",
    "      \"text\": [\n",
    "       \"Tokenization test:\\n\",\n",
    "       \"<|begin_of_text|>Below is the first 45 seconds of the sales call between the sales agent Speaker 0 and the customer Speaker 1:\\n\",\n",
    "       \"Speaker 0: Good afternoon, this is Marcus from Greenwave Energy. Am I speaking with Ms. Lopez?\\n\",\n",
    "       \"Speaker 1: Hi, yes, this is her.\\n\",\n",
    "       \"Speaker 0: Fantastic! I’ll keep this brief. We have a new energy plan with a guaranteed rate and monthly discounts for loyal clients. Are you open to hearing a quick summary?\\n\",\n",
    "       \"Speaker 1: Alright, sure. Go ahead.\\n\",\n",
    "       \"Speaker 0: Thank you! With the Greenwave Saver Plan, you lock in a fixed rate on electricity for a year. We're offering a $7 discount each month on your bill and a one-time $30 sign-up bonus. All this, with no contract lock-in or exit fees.\\n\",\n",
    "       \"Speaker 1: Hmm. What's the rate compared to what I'm paying now?\\n\",\n",
    "       \"Speaker 0: Great question. On your most recent bill, you were charged $0.15 per kWh. Our plan offers $0.132 per kWh, so you'd see a savings, plus the ongoing monthly discount.\\n\",\n",
    "       \"Speaker 1: I don’t know… It sounds good, but I just switched providers last month. I’m kind of locked in for now.\\n\",\n",
    "       \"Speaker 0: I understand. If there’s a penalty to leave early, I’d hate for you to pay that. Just so you know, our plan has no switching fee, so you could come back anytime.\\n\",\n",
    "       \"Speaker 1: That’s helpful, thank you. But I think for now, I’ll have to pass. Maybe I’ll look into it when my contact runs out.\\n\",\n",
    "       \"Will this call end in a sale (respond with 'yes' or 'no'):  no<|end_of_text|>\\n\",\n",
    "       \"\\n\",\n",
    "       \"Expected Label (action):\\n\",\n",
    "       \"no<|end_of_text|>\\n\"\n",
    "      ]\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Builds train/val datasets and tokenizes prompts+labels with manual masking.\\n\",\n",
    "+    \"# Why we choose this setup: Manual label masking trains only on the action token(s), and MAX_SEQ_LEN truncation prevents long transcripts from exhausting memory.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"train_dataset = datasets.Dataset.from_dict(\\n\",\n",
    "     \"    {\\\"prompt\\\": [state for state in optimal_state_action_pairs[\\\"train\\\"][\\\"state\\\"].values], \\n\",\n",
    "     \"     \\\"completion\\\": [action.strip()\\n\",\n",
    "     \"                    for action in optimal_state_action_pairs[\\\"train\\\"][\\\"action\\\"].values]}).shuffle()\\n\",\n",
    "     \"    \\n\",\n",
    "     \"val_dataset = datasets.Dataset.from_dict(\\n\",\n",
    "     \"    {\\\"prompt\\\": [state for state in optimal_state_action_pairs[\\\"val\\\"][\\\"state\\\"].values],\\n\",\n",
    "     \"     \\\"completion\\\": [action.strip()\\n\",\n",
    "     \"                    for action in optimal_state_action_pairs[\\\"val\\\"][\\\"action\\\"].values]}).shuffle()\\n\",\n",
    "     \"\\n\",\n",
    "     \"def tokenize_fn(example, add_label):\\n\",\n",
    "     \"    # start with the BOS token if it exists\\n\",\n",
    "     \"    if tokenizer.bos_token is not None:\\n\",\n",
    "     \"        encoded_prompt = tokenizer.encode(tokenizer.bos_token +\\n\",\n",
    "     \"                                          example[\\\"prompt\\\"],              \\n\",\n",
    "     \"                                          add_special_tokens=False)\\n\",\n",
    "     \"    else:\\n\",\n",
    "     \"        encoded_prompt = tokenizer.encode(example[\\\"prompt\\\"], \\n\",\n",
    "     \"                                          add_special_tokens=False)\\n\",\n",
    "     \"\\n\",\n",
    "     \"    # add the label if needed for the training and validation datasets\\n\",\n",
    "     \"    if add_label:\\n\",\n",
    "     \"        encoded_label = tokenizer.encode(example[\\\"completion\\\"] + tokenizer.eos_token, \\n\",\n",
    "     \"                                         add_special_tokens=False)\\n\",\n",
    "-    \"        return {\\\"input_ids\\\": encoded_prompt + encoded_label,\\n\",\n",
    "-    \"                \\\"attention_mask\\\" : [1] * (len(encoded_prompt) + len(encoded_label)),\\n\",\n",
    "-    \"                \\\"labels\\\": [-100] * len(encoded_prompt) + encoded_label}\\n\",\n",
    "+    \"        input_ids = (encoded_prompt + encoded_label)[:MAX_SEQ_LEN]\\n\",\n",
    "+    \"        labels = ([-100] * len(encoded_prompt) + encoded_label)[:MAX_SEQ_LEN]\\n\",\n",
    "+    \"        return {\\\"input_ids\\\": input_ids,\\n\",\n",
    "+    \"                \\\"attention_mask\\\" : [1] * len(input_ids),\\n\",\n",
    "+    \"                \\\"labels\\\": labels}\\n\",\n",
    "     \"    else:\\n\",\n",
    "-    \"        return {\\\"input_ids\\\": encoded_prompt,\\n\",\n",
    "-    \"                \\\"attention_mask\\\": [1] * len(encoded_prompt),\\n\",\n",
    "-    \"                \\\"labels\\\": [-100] * len(encoded_prompt)}\\n\",\n",
    "+    \"        input_ids = encoded_prompt[:MAX_SEQ_LEN]\\n\",\n",
    "+    \"        return {\\\"input_ids\\\": input_ids,\\n\",\n",
    "+    \"                \\\"attention_mask\\\": [1] * len(input_ids),\\n\",\n",
    "+    \"                \\\"labels\\\": [-100] * len(input_ids)}\\n\",\n",
    "     \"\\n\",\n",
    "     \"train_dataset = train_dataset.map(tokenize_fn,\\n\",\n",
    "     \"                                  remove_columns=[\\\"prompt\\\", \\\"completion\\\"], \\n\",\n",
    "     \"                                  fn_kwargs={\\\"add_label\\\": True})\\n\",\n",
    "     \"val_dataset = val_dataset.map(tokenize_fn, \\n\",\n",
    "     \"                              remove_columns=[\\\"prompt\\\", \\\"completion\\\"], \\n\",\n",
    "     \"                              fn_kwargs={\\\"add_label\\\": True})\\n\",\n",
    "     \"\\n\",\n",
    "     \"print(\\\"Tokenization test:\\\")\\n\",\n",
    "     \"print(tokenizer.decode(train_dataset[0][\\\"input_ids\\\"]))\\n\",\n",
    "     \"print()\\n\",\n",
    "     \"print(\\\"Expected Label (action):\\\")\\n\",\n",
    "     \"print(tokenizer.decode([l for l in train_dataset[0][\\\"labels\\\"] if l!=-100]))\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"markdown\",\n",
    "    \"id\": \"8f5717d6\",\n",
    "    \"metadata\": {},\n",
    "    \"source\": [\n",
    "     \"### Fine-tune\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"code\",\n",
    "@@ -930,119 +984,121 @@\n",
    "        \"      <td>0.329565</td>\\n\",\n",
    "        \"      <td>1.000000</td>\\n\",\n",
    "        \"    </tr>\\n\",\n",
    "        \"  </tbody>\\n\",\n",
    "        \"</table><p>\"\n",
    "       ],\n",
    "       \"text/plain\": [\n",
    "        \"<IPython.core.display.HTML object>\"\n",
    "       ]\n",
    "      },\n",
    "      \"metadata\": {},\n",
    "      \"output_type\": \"display_data\"\n",
    "     },\n",
    "     {\n",
    "      \"data\": {\n",
    "       \"text/plain\": [\n",
    "        \"TrainOutput(global_step=75, training_loss=1.3294019158681234, metrics={'train_runtime': 144.6715, 'train_samples_per_second': 12.096, 'train_steps_per_second': 1.037, 'total_flos': 7403051037947904.0, 'train_loss': 1.3294019158681234, 'epoch': 5.0})\"\n",
    "       ]\n",
    "      },\n",
    "      \"execution_count\": 9,\n",
    "      \"metadata\": {},\n",
    "      \"output_type\": \"execute_result\"\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Defines metrics and launches Trainer fine-tuning with early stopping.\\n\",\n",
    "+    \"# Why we choose this setup: Small batches + gradient accumulation/checkpointing are memory-safe defaults that still preserve effective batch size and validation-driven model selection.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"YES_TOKEN_ID = tokenizer.encode(\\\"yes\\\", add_special_tokens=False)[-1]\\n\",\n",
    "     \"NO_TOKEN_ID = tokenizer.encode(\\\"no\\\", add_special_tokens=False)[-1]\\n\",\n",
    "     \"\\n\",\n",
    "     \"def preprocess_logits_for_metrics(logits, labels):\\n\",\n",
    "     \"    \\\"\\\"\\\"\\n\",\n",
    "     \"    Original Trainer may have a memory leak. \\n\",\n",
    "     \"    This is a workaround to avoid storing too many tensors that are not needed.\\n\",\n",
    "     \"    \\\"\\\"\\\"\\n\",\n",
    "     \"    return logits[:, -3, :] # last non-padding token logits only, for causal LM\\n\",\n",
    "     \"\\n\",\n",
    "     \"def compute_metrics(eval_preds):\\n\",\n",
    "     \"    logits, labels = eval_preds\\n\",\n",
    "     \"    labels = [l[(l != -100) & (l != tokenizer.pad_token_id)][0] for l in labels]\\n\",\n",
    "     \"\\n\",\n",
    "     \"    logprobs = [torch.log_softmax(torch.from_numpy(s), dim=-1).numpy()\\n\",\n",
    "     \"                for s in logits]\\n\",\n",
    "     \"    labelprobs = [math.exp(logprob[label]) for logprob, label in zip(logprobs, labels)]\\n\",\n",
    "     \"\\n\",\n",
    "     \"    ytrue = [1 if label == YES_TOKEN_ID else 0 for label in labels]\\n\",\n",
    "     \"    ypred = [labelprob if label==YES_TOKEN_ID else 1.0 - labelprob\\n\",\n",
    "     \"             for labelprob, label in zip(labelprobs, labels)]\\n\",\n",
    "     \"    auc = roc_auc_score(ytrue, ypred)\\n\",\n",
    "     \"\\n\",\n",
    "     \"    return {\\\"auc\\\": auc}\\n\",\n",
    "     \"\\n\",\n",
    "     \"training_args = transformers.TrainingArguments(\\n\",\n",
    "-    \"    output_dir=\\\"./llama-3.2-3B/\\\",\\n\",\n",
    "+    \"    output_dir=\\\"./llama-3.2/\\\",\\n\",\n",
    "     \"    overwrite_output_dir=True,\\n\",\n",
    "     \"    remove_unused_columns=False,\\n\",\n",
    "     \"\\n\",\n",
    "     \"    save_strategy=\\\"best\\\",\\n\",\n",
    "     \"    logging_strategy=\\\"epoch\\\",\\n\",\n",
    "     \"    eval_strategy=\\\"epoch\\\",\\n\",\n",
    "     \"    save_total_limit=1,\\n\",\n",
    "     \"\\n\",\n",
    "-    \"    # on 1 H100 96GB: batch size of 12-16 for 3B works\\n\",\n",
    "-    \"    per_device_train_batch_size=12,\\n\",\n",
    "-    \"    per_device_eval_batch_size=12,\\n\",\n",
    "-    \"    gradient_accumulation_steps=1,\\n\",\n",
    "+    \"    per_device_train_batch_size=TRAIN_BATCH_SIZE,\\n\",\n",
    "+    \"    per_device_eval_batch_size=EVAL_BATCH_SIZE,\\n\",\n",
    "+    \"    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\\n\",\n",
    "     \"\\n\",\n",
    "     \"    num_train_epochs=10,\\n\",\n",
    "     \"    learning_rate=1e-4,\\n\",\n",
    "     \"    optim=\\\"adamw_torch\\\",\\n\",\n",
    "+    \"    gradient_checkpointing=LOW_RAM_MODE,\\n\",\n",
    "     \"\\n\",\n",
    "     \"    load_best_model_at_end=True,\\n\",\n",
    "     \"    metric_for_best_model=\\\"auc\\\",\\n\",\n",
    "     \"    greater_is_better=True,\\n\",\n",
    "     \"\\n\",\n",
    "     \"    report_to=\\\"none\\\", # change to wandb if needed\\n\",\n",
    "     \"    save_safetensors=False, # needed to load saved models\\n\",\n",
    "     \"\\n\",\n",
    "-    \"    # change to suit hardware\\n\",\n",
    "-    \"    bf16=True, \\n\",\n",
    "-    \"    fp16=False,\\n\",\n",
    "+    \"    bf16=torch.cuda.is_available() and not LOW_RAM_MODE,\\n\",\n",
    "+    \"    fp16=torch.cuda.is_available() and LOW_RAM_MODE,\\n\",\n",
    "     \")\\n\",\n",
    "     \"\\n\",\n",
    "     \"trainer = transformers.Trainer(\\n\",\n",
    "     \"    model=model,\\n\",\n",
    "     \"    args=training_args,\\n\",\n",
    "     \"    train_dataset=train_dataset.shuffle(),\\n\",\n",
    "     \"    eval_dataset=val_dataset,\\n\",\n",
    "     \"    processing_class=tokenizer,\\n\",\n",
    "-    \"    data_collator=transformers.data.DataCollatorForSeq2Seq(tokenizer),\\n\",\n",
    "+    \"    data_collator=transformers.data.DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None),\\n\",\n",
    "     \"    compute_metrics=compute_metrics,\\n\",\n",
    "     \"    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\\n\",\n",
    "     \"    callbacks=[transformers.EarlyStoppingCallback(early_stopping_patience=1)]\\n\",\n",
    "     \")\\n\",\n",
    "     \"\\n\",\n",
    "-    \"trainer.train(resume_from_checkpoint=False)\"\n",
    "+    \"trainer.train(resume_from_checkpoint=False)\\n\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"markdown\",\n",
    "    \"id\": \"56c26914\",\n",
    "    \"metadata\": {},\n",
    "    \"source\": [\n",
    "     \"### Evaluate\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"markdown\",\n",
    "    \"id\": \"cdb95891\",\n",
    "    \"metadata\": {},\n",
    "    \"source\": [\n",
    "     \"#### Get the validation and test responses for each state\\n\",\n",
    "     \"\\n\",\n",
    "     \"We get the agent's responses for the validation and test conversations. We use the validation responses for backward-induction threshold-tuning (our scalable alternative to grid search).\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"code\",\n",
    "    \"execution_count\": 18,\n",
    "    \"id\": \"88183bd6\",\n",
    "    \"metadata\": {},\n",
    "@@ -1069,159 +1125,169 @@\n",
    "      \"output_type\": \"display_data\"\n",
    "     },\n",
    "     {\n",
    "      \"name\": \"stdout\",\n",
    "      \"output_type\": \"stream\",\n",
    "      \"text\": [\n",
    "       \"Getting validation responses...\\n\"\n",
    "      ]\n",
    "     },\n",
    "     {\n",
    "      \"data\": {\n",
    "       \"application/vnd.jupyter.widget-view+json\": {\n",
    "        \"model_id\": \"89a699eea10245818047fa37bed6ec25\",\n",
    "        \"version_major\": 2,\n",
    "        \"version_minor\": 0\n",
    "       },\n",
    "       \"text/plain\": [\n",
    "        \"  0%|          | 0/1 [00:00<?, ?it/s]\"\n",
    "       ]\n",
    "      },\n",
    "      \"metadata\": {},\n",
    "      \"output_type\": \"display_data\"\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Runs batched generation on validation/test prompts and collects response log-probabilities.\\n\",\n",
    "+    \"# Why we choose this setup: Controlled greedy decoding gives deterministic policy outputs, and reduced generation batches avoid inference-time OOMs.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"val_prompts = list(optimal_state_action_pairs[\\\"val\\\"][\\\"state\\\"].values)\\n\",\n",
    "     \"test_prompts = list(optimal_state_action_pairs[\\\"test\\\"][\\\"state\\\"].values)\\n\",\n",
    "     \"\\n\",\n",
    "     \"print(\\\"Getting test responses...\\\")\\n\",\n",
    "     \"\\n\",\n",
    "     \"responses_test = []\\n\",\n",
    "     \"logprobs_test = []\\n\",\n",
    "-    \"batch_size = 72 # change to suit hardware\\n\",\n",
    "+    \"batch_size = GEN_BATCH_SIZE # lower for Colab/laptops\\n\",\n",
    "     \"\\n\",\n",
    "     \"for i in tqdm(range(0, len(test_prompts), batch_size)):\\n\",\n",
    "     \"    batch_prompts = test_prompts[i:i+batch_size]\\n\",\n",
    "     \"    batch = tokenizer(batch_prompts, \\n\",\n",
    "     \"                      return_tensors=\\\"pt\\\", \\n\",\n",
    "     \"                      padding=True, \\n\",\n",
    "     \"                      add_special_tokens=True,\\n\",\n",
    "     \"                      truncation=True).to(\\\"cuda\\\")\\n\",\n",
    "     \"\\n\",\n",
    "     \"    with torch.no_grad():\\n\",\n",
    "     \"        outputs = trainer.model.generate(\\n\",\n",
    "     \"            **batch, \\n\",\n",
    "     \"            max_new_tokens=2, \\n\",\n",
    "     \"            do_sample=False,\\n\",\n",
    "     \"            pad_token_id=tokenizer.eos_token_id,\\n\",\n",
    "     \"            temperature=None, top_p=None, top_k=None,\\n\",\n",
    "     \"            return_dict_in_generate=True, output_scores=True\\n\",\n",
    "     \"            # greedy decoding: so output_scores = output_logits\\n\",\n",
    "     \"        )\\n\",\n",
    "     \"\\n\",\n",
    "     \"    seqs = outputs.sequences\\n\",\n",
    "     \"    prompt_len = batch['input_ids'].shape[1]   # Length of the input prompts\\n\",\n",
    "     \"\\n\",\n",
    "     \"    # Slice to get only the generated new tokens\\n\",\n",
    "     \"    generated_tokens = seqs[:, prompt_len:]\\n\",\n",
    "     \"    decoded_outputs = tokenizer.batch_decode(generated_tokens,\\n\",\n",
    "     \"                                             skip_special_tokens=True)\\n\",\n",
    "     \"    decoded_outputs = [d.strip().lower() for d in decoded_outputs]\\n\",\n",
    "     \"\\n\",\n",
    "     \"    responses_test.extend(decoded_outputs)\\n\",\n",
    "     \"\\n\",\n",
    "     \"    scores = outputs.scores\\n\",\n",
    "     \"    logprobs = [torch.log_softmax(s, dim=-1) for s in scores]\\n\",\n",
    "     \"    logprobs = logprobs[0][torch.arange(logprobs[0].size(0)), seqs[:, -2].view(-1)]\\n\",\n",
    "     \"    logprobs_test.extend(logprobs.cpu().numpy())\\n\",\n",
    "     \"\\n\",\n",
    "     \"print(\\\"Getting validation responses...\\\")\\n\",\n",
    "     \"\\n\",\n",
    "     \"responses_val = []\\n\",\n",
    "     \"logprobs_val = []\\n\",\n",
    "-    \"batch_size = 72\\n\",\n",
    "+    \"batch_size = GEN_BATCH_SIZE\\n\",\n",
    "     \"\\n\",\n",
    "     \"for i in tqdm(range(0, len(val_prompts), batch_size)):\\n\",\n",
    "     \"    batch_prompts = val_prompts[i:i+batch_size]\\n\",\n",
    "     \"    batch = tokenizer(batch_prompts, \\n\",\n",
    "     \"                      return_tensors=\\\"pt\\\", \\n\",\n",
    "     \"                      padding=True, \\n\",\n",
    "     \"                      truncation=True).to(\\\"cuda\\\")\\n\",\n",
    "     \"\\n\",\n",
    "     \"    with torch.no_grad():\\n\",\n",
    "     \"        outputs = trainer.model.generate(\\n\",\n",
    "     \"            **batch, \\n\",\n",
    "     \"            max_new_tokens=2, \\n\",\n",
    "     \"            do_sample=False,\\n\",\n",
    "     \"            pad_token_id=tokenizer.eos_token_id,\\n\",\n",
    "     \"            temperature=None, top_p=None, top_k=None,\\n\",\n",
    "     \"            return_dict_in_generate=True, output_scores=True\\n\",\n",
    "     \"        )\\n\",\n",
    "     \"\\n\",\n",
    "     \"    seqs = outputs.sequences\\n\",\n",
    "     \"    prompt_len = batch['input_ids'].shape[1]   # Length of the input prompts\\n\",\n",
    "     \"\\n\",\n",
    "     \"    # Slice to get only the generated new tokens\\n\",\n",
    "     \"    generated_tokens = seqs[:, prompt_len:]\\n\",\n",
    "     \"    decoded_outputs = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\\n\",\n",
    "     \"    decoded_outputs = [d.strip().lower() for d in decoded_outputs]\\n\",\n",
    "     \"\\n\",\n",
    "     \"    responses_val.extend(decoded_outputs)\\n\",\n",
    "     \"\\n\",\n",
    "     \"    scores = outputs.scores\\n\",\n",
    "     \"    logprobs = [torch.log_softmax(s, dim=-1) for s in scores]\\n\",\n",
    "     \"    logprobs = logprobs[0][torch.arange(logprobs[0].size(0)), seqs[:, -2].view(-1)]\\n\",\n",
    "-    \"    logprobs_val.extend(logprobs.cpu().numpy())\"\n",
    "+    \"    logprobs_val.extend(logprobs.cpu().numpy())\\n\",\n",
    "+    \"# Free memory before threshold tuning\\n\",\n",
    "+    \"gc.collect()\\n\",\n",
    "+    \"if torch.cuda.is_available():\\n\",\n",
    "+    \"    torch.cuda.empty_cache()\\n\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"markdown\",\n",
    "    \"id\": \"86c7a682\",\n",
    "    \"metadata\": {},\n",
    "    \"source\": [\n",
    "     \"#### Store validation and test responses\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"code\",\n",
    "    \"execution_count\": 20,\n",
    "    \"id\": \"772d7131\",\n",
    "    \"metadata\": {},\n",
    "    \"outputs\": [\n",
    "     {\n",
    "      \"name\": \"stdout\",\n",
    "      \"output_type\": \"stream\",\n",
    "      \"text\": [\n",
    "       \"Val 45 ROC-AUC: 1.00\\n\",\n",
    "       \"Test 45 ROC-AUC: 0.99\\n\",\n",
    "       \"Val 60 ROC-AUC: 1.00\\n\",\n",
    "       \"Test 60 ROC-AUC: 0.98\\n\"\n",
    "      ]\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Converts generation outputs into calibrated yes-probabilities and reports ROC-AUC by decision time.\\n\",\n",
    "+    \"# Why we choose this setup: Probability-based evaluation is needed for downstream threshold tuning and gives a robust quality check beyond raw accuracy.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"predictions = optimal_state_action_pairs[\\\"test\\\"].copy()\\n\",\n",
    "     \"predictions[\\\"response\\\"] = responses_test\\n\",\n",
    "     \"predictions[\\\"logprob\\\"] = logprobs_test\\n\",\n",
    "     \"predictions[\\\"prob\\\"] = predictions[\\\"logprob\\\"].apply(lambda x: math.exp(x))\\n\",\n",
    "     \"predictions.loc[predictions[\\\"response\\\"]==\\\"yes\\\", \\\"prob_yes\\\"] =\\\\\\n\",\n",
    "     \"    predictions.loc[predictions[\\\"response\\\"]==\\\"yes\\\", \\\"prob\\\"]\\n\",\n",
    "     \"predictions.loc[predictions[\\\"response\\\"]!=\\\"yes\\\", \\\"prob_yes\\\"] =\\\\\\n\",\n",
    "     \"    1.0 - predictions.loc[predictions[\\\"response\\\"]!=\\\"yes\\\", \\\"prob\\\"]\\n\",\n",
    "     \"\\n\",\n",
    "     \"predictions_val = optimal_state_action_pairs[\\\"val\\\"].copy()\\n\",\n",
    "     \"predictions_val[\\\"response\\\"] = responses_val\\n\",\n",
    "     \"predictions_val[\\\"logprob\\\"] = logprobs_val\\n\",\n",
    "     \"predictions_val[\\\"prob\\\"] =\\\\\\n\",\n",
    "     \"    predictions_val[\\\"logprob\\\"].apply(lambda x: math.exp(x))\\n\",\n",
    "     \"predictions_val.loc[predictions_val[\\\"response\\\"]==\\\"yes\\\", \\\"prob_yes\\\"] =\\\\\\n\",\n",
    "     \"    predictions_val.loc[predictions_val[\\\"response\\\"]==\\\"yes\\\", \\\"prob\\\"]\\n\",\n",
    "     \"predictions_val.loc[predictions_val[\\\"response\\\"]!=\\\"yes\\\", \\\"prob_yes\\\"] =\\\\\\n\",\n",
    "     \"    1.0 - predictions_val.loc[predictions_val[\\\"response\\\"]!=\\\"yes\\\", \\\"prob\\\"]\\n\",\n",
    "     \"\\n\",\n",
    "     \"test_with_predictions  = pd.merge(left=data_transcripts[\\\"test\\\"],\\n\",\n",
    "     \"                                     right=predictions[[\\\"conversation_id\\\",\\n\",\n",
    "     \"                                                        \\\"state\\\", \\\"prob_yes\\\"]],\\n\",\n",
    "     \"                                     left_on=[\\\"conversation_id\\\", \\\"s\\\" + str(m1)],\\n\",\n",
    "     \"                                     right_on=[\\\"conversation_id\\\", \\\"state\\\"],\\n\",\n",
    "     \"                                     how=\\\"left\\\", validate=\\\"one_to_one\\\")\\\\\\n\",\n",
    "@@ -1287,50 +1353,53 @@\n",
    "    \"source\": [\n",
    "     \"#### Get optimal thresholds using backward-induction threshold tuning\\n\",\n",
    "     \"\\n\",\n",
    "     \"We could do a grid search, but this is much faster.\"\n",
    "    ]\n",
    "   },\n",
    "   {\n",
    "    \"cell_type\": \"code\",\n",
    "    \"execution_count\": 56,\n",
    "    \"id\": \"4f728823\",\n",
    "    \"metadata\": {},\n",
    "    \"outputs\": [\n",
    "     {\n",
    "      \"name\": \"stdout\",\n",
    "      \"output_type\": \"stream\",\n",
    "      \"text\": [\n",
    "       \"Test set reward without the stopping agent:\\n\",\n",
    "       \"Total reward on test: -77.97900000000004\\n\",\n",
    "       \"Avg. reward on test: -1.5595800000000009\\n\",\n",
    "       \"Total sales on test: 25\\n\",\n",
    "       \"Total time on test (seconds): 3279.79\\n\"\n",
    "      ]\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Simulates policy reward under threshold choices at m1/m2 and finds validation-optimal thresholds.\\n\",\n",
    "+    \"# Why we choose this setup: Backward-induction-style threshold tuning scales better than brute-force policy search while matching the stopping objective.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"def simulate_threshold(threshold_m1, threshold_m2, df):\\n\",\n",
    "     \"    # quit at m1\\n\",\n",
    "     \"    calls_quit_at_m1 = df.loc[(df[\\\"prob_yes_\\\" + str(m1)] < threshold_m1)]\\n\",\n",
    "     \"    \\n\",\n",
    "     \"    # continue at m1, ended before m2\\n\",\n",
    "     \"    calls_continued_at_m1_and_ended = df.loc[(df[\\\"prob_yes_\\\" + str(m1)] >= threshold_m1) &\\n\",\n",
    "     \"                                             (df[\\\"duration\\\"]<m2)]\\n\",\n",
    "     \"    \\n\",\n",
    "     \"    # continued at m1, did not end before m2, quit at m2\\n\",\n",
    "     \"    calls_continued_at_m1_and_quit_at_m2 = df.loc[(df[\\\"prob_yes_\\\" + str(m1)] >= threshold_m1) &\\n\",\n",
    "     \"                                                  (df[\\\"prob_yes_\\\" + str(m2)] < threshold_m2) &\\n\",\n",
    "     \"                                                  (df[\\\"duration\\\"]>=m2)]\\n\",\n",
    "     \"    \\n\",\n",
    "     \"    # continue at m1, did not end before m2, continued at m2\\n\",\n",
    "     \"    calls_continued_at_m2 = df.loc[(df[\\\"prob_yes_\\\" + str(m1)] >= threshold_m1) &\\n\",\n",
    "     \"                                   (df[\\\"prob_yes_\\\" + str(m2)] >= threshold_m2) &\\n\",\n",
    "     \"                                   (df[\\\"duration\\\"]>=m2)]\\n\",\n",
    "     \"    \\n\",\n",
    "     \"    assert len(calls_quit_at_m1) + len(calls_continued_at_m1_and_ended) +\\\\\\n\",\n",
    "     \"          len(calls_continued_at_m1_and_quit_at_m2) + \\\\\\n\",\n",
    "     \"          len(calls_continued_at_m2) == len(df)\\n\",\n",
    "     \"\\n\",\n",
    "     \"    total_sales = calls_continued_at_m1_and_ended[\\\"is_sale\\\"].sum() +\\\\\\n\",\n",
    "     \"                    calls_continued_at_m2[\\\"is_sale\\\"].sum()\\n\",\n",
    "     \"    total_sales_benefit = total_sales * BENEFIT_PER_POSITIVE_OUTCOME\\n\",\n",
    "@@ -1372,50 +1441,53 @@\n",
    "    \"execution_count\": 61,\n",
    "    \"id\": \"85ea3128\",\n",
    "    \"metadata\": {},\n",
    "    \"outputs\": [\n",
    "     {\n",
    "      \"name\": \"stdout\",\n",
    "      \"output_type\": \"stream\",\n",
    "      \"text\": [\n",
    "       \"Best threshold at m=45: 0.00010000464725449688\\n\",\n",
    "       \"Best threshold at m=60: 0.0014000817639441545\\n\",\n",
    "       \"\\n\",\n",
    "       \"Test set reward with the stopping agent:\\n\",\n",
    "       \"Total reward on test: -60.261000000000024\\n\",\n",
    "       \"Avg. reward on test: -1.2052200000000004\\n\",\n",
    "       \"Total sales on test: 24\\n\",\n",
    "       \"Total time on test: 3002.61\\n\",\n",
    "       \"\\n\",\n",
    "       \"Comparative results:\\n\",\n",
    "       \"Sales lost by stopping agent: 1\\n\",\n",
    "       \"Time saved by stopping agent (seconds): 277.17999999999984\\n\",\n",
    "       \"Time saved by stopping agent (%): 8.451150835876682\\n\"\n",
    "      ]\n",
    "     }\n",
    "    ],\n",
    "    \"source\": [\n",
    "+    \"# What this block does: Evaluates final thresholded policy on test calls and summarizes action frequencies and achieved reward.\\n\",\n",
    "+    \"# Why we choose this setup: This is the end-to-end objective metric—how much reward the learned stopping policy yields on unseen conversations.\\n\",\n",
    "+    \"\\n\",\n",
    "     \"best_threshold_at_m = {}\\n\",\n",
    "     \"num_grid_points = 10000\\n\",\n",
    "     \"\\n\",\n",
    "     \"m = m1\\n\",\n",
    "     \"prob_column = \\\"prob_yes_\\\" + str(m)    \\n\",\n",
    "     \"best_reward = -10000000\\n\",\n",
    "     \"for candidate_threshold in np.linspace(val_with_predictions[prob_column].min()-10**-12,\\n\",\n",
    "     \"                                       val_with_predictions[prob_column].max()+10**-12,\\n\",\n",
    "     \"                                       num=num_grid_points):\\n\",\n",
    "     \"    \\n\",\n",
    "     \"    total_reward, average_reward, total_sales, total_time =\\\\\\n\",\n",
    "     \"        simulate_threshold(0, candidate_threshold, val_with_predictions)\\n\",\n",
    "     \"    \\n\",\n",
    "     \"    if average_reward > best_reward:\\n\",\n",
    "     \"        best_reward = average_reward\\n\",\n",
    "     \"        best_threshold_at_m[m] = candidate_threshold\\n\",\n",
    "     \"\\n\",\n",
    "     \"print(\\\"Best threshold at m=\\\" + str(m) + \\\":\\\", best_threshold_at_m[m])\\n\",\n",
    "     \"\\n\",\n",
    "     \"m = m2\\n\",\n",
    "     \"prob_column = \\\"prob_yes_\\\" + str(m)    \\n\",\n",
    "     \"best_reward = -10000000\\n\",\n",
    "     \"for candidate_threshold in np.linspace(val_with_predictions[prob_column].min()-10**-12,\\n\",\n",
    "     \"                                       val_with_predictions[prob_column].max()+10**-12,\\n\",\n",
    "     \"                                       num=num_grid_points):\\n\",\n",
    "''', encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Wrote patch to: {patch_path.resolve()}\")\n",
    "print(\"Next (from your repo root): git apply path/to/optimal-stopping.patch\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
