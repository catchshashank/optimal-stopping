{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Llama 3.2 1B + Behavioral Cloning**\n",
        "\n",
        "Replicated by [Shashank Dubey](https://github.com/catchshashank) to the [stopping-agents](https://github.com/catchshashank/optimal-stopping) repository."
      ],
      "metadata": {
        "id": "Wgfm61csumQy"
      },
      "id": "Wgfm61csumQy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Import Dependencies**"
      ],
      "metadata": {
        "id": "db5w_xfozOkV"
      },
      "id": "db5w_xfozOkV"
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import huggingface_hub # needed for Llama models\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "RTbwUX1jzRs3"
      },
      "id": "RTbwUX1jzRs3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Set parameters**"
      ],
      "metadata": {
        "id": "-b8h_blTzWha"
      },
      "id": "-b8h_blTzWha"
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN = \"HF_TOKEN\"\n",
        "\n",
        "COST_PER_UNIT_TIME = 0.1\n",
        "BENEFIT_PER_POSITIVE_OUTCOME = 10.0\n",
        "DECISION_OPPORTUNITIES = [45, 60] # time in seconds at which the\n",
        "                                  # agent can decide to quit or wait\n",
        "                                  # code is tailored to just 2 right now"
      ],
      "metadata": {
        "id": "8LWW7emrzUeU"
      },
      "id": "8LWW7emrzUeU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Load Data + Define df**"
      ],
      "metadata": {
        "id": "Uc_PVdzt0LWt"
      },
      "id": "Uc_PVdzt0LWt"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_url = \"https://raw.githubusercontent.com/emaadmanzoor/stopping-agents/refs/heads/main/datasets/synthetic_sales_conversations.csv?token=GHSAT0AAAAAADBUAD4WOA6XRF2GSIX5UC4Y2EEF66Q\"\n",
        "\n",
        "diarized_conversations = pd.read_csv(dataset_url)\n",
        "\n",
        "# Add column (\"is_sale or not\")\n",
        "diarized_conversations[\"is_sale\"] =\\\n",
        "        diarized_conversations[\"outcome\"].apply(\n",
        "            lambda x: 1 if x == \"sale\" else 0 if x == \"no sale\" else np.nan)\n",
        "\n",
        "# Add column (\"duration\")\n",
        "diarized_conversations[\"duration\"] =\\\n",
        "    diarized_conversations.groupby(\"conversation_id\")[\"end_time\"].transform(\"max\")\n",
        "\n",
        "diarized_conversations.head()"
      ],
      "metadata": {
        "id": "CNIrBHPF4q-F"
      },
      "id": "CNIrBHPF4q-F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Split into train, validation, and test conversations**"
      ],
      "metadata": {
        "id": "h5Aaoxj35-6Y"
      },
      "id": "h5Aaoxj35-6Y"
    },
    {
      "cell_type": "code",
      "source": [
        "all_conversation_ids =\\\n",
        "    diarized_conversations[[\"conversation_id\", \"is_sale\"]].drop_duplicates()[\"conversation_id\"].values\n",
        "all_outcomes =\\\n",
        "    diarized_conversations[[\"conversation_id\", \"is_sale\"]].drop_duplicates()[\"is_sale\"].values\n",
        "\n",
        "# Split dataset into train + test datasets ensuring class balance\n",
        "train_conversation_ids, test_conversation_ids, train_outcomes, test_outcomes =\\\n",
        "    train_test_split(all_conversation_ids, all_outcomes, test_size=0.25, random_state=42,\n",
        "                     stratify=all_outcomes)\n",
        "\n",
        "# Further split training data into train + val dataset\n",
        "train_conversation_ids, val_conversation_ids, train_outcomes, val_outcomes =\\\n",
        "    train_test_split(train_conversation_ids, train_outcomes, test_size=0.25, random_state=42,\n",
        "                     stratify=train_outcomes)\n",
        "\n",
        "# Final diarized conversation datasets = train + val + test (25%)\n",
        "diarized_conversations_train =\\\n",
        "    diarized_conversations[diarized_conversations[\"conversation_id\"].isin(train_conversation_ids)]\n",
        "diarized_conversations_val =\\\n",
        "    diarized_conversations[diarized_conversations[\"conversation_id\"].isin(val_conversation_ids)]\n",
        "diarized_conversations_test =\\\n",
        "    diarized_conversations[diarized_conversations[\"conversation_id\"].isin(test_conversation_ids)]\n",
        "\n",
        "print(len(diarized_conversations_train), \"train conversations.\")\n",
        "print(len(diarized_conversations_val), \"validation conversations.\")\n",
        "print(len(diarized_conversations_test), \"test conversations.\")"
      ],
      "metadata": {
        "id": "eCfnQASW6Dt7"
      },
      "id": "eCfnQASW6Dt7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}