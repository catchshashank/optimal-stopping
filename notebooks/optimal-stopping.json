diff --git a/notebooks/model-training + Behav Cloning.ipynb b/notebooks/model-training + Behav Cloning.ipynb
index ac4b69a5e9d865f3f2baa48862ee346b15160954..c468f8fc979bb1d188a295e7dc0e253b024a9ed4 100644
--- a/notebooks/model-training + Behav Cloning.ipynb	
+++ b/notebooks/model-training + Behav Cloning.ipynb	
@@ -18,68 +18,77 @@
     "## Imports\n",
     "\n",
     "Before running this notebook locally, you need to [install PyTorch](https://pytorch.org/get-started/locally/) for your hardware.\n",
     "\n",
     "Then, you need to install the following packages:\n",
     "\n",
     "   * transformers\n",
     "   * datasets\n",
     "   * accelerate\n",
     "   * pandas\n",
     "   * huggingface_hub (needed for Llama models)\n",
     "   * scikit-learn\n",
     "   * numpy\n",
     "\n",
     "You an also use the `requirements.txt` in the [stopping-agents](https://github.com/emaadmanzoor/stopping-agents) repository."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": null,
    "id": "cf5fb535",
    "metadata": {},
    "outputs": [],
    "source": [
     "import datasets\n",
+    "import gc\n",
     "import huggingface_hub # needed for Llama models\n",
     "import math\n",
     "import numpy as np\n",
+    "import os\n",
     "import pandas as pd\n",
     "import torch\n",
     "import transformers\n",
     "\n",
     "from sklearn.model_selection import train_test_split\n",
     "from sklearn.metrics import roc_auc_score\n",
     "from tqdm.auto import tqdm\n",
     "\n",
     "HF_TOKEN = \"HF_TOKEN\"\n",
     "\n",
     "COST_PER_UNIT_TIME = 0.1\n",
     "BENEFIT_PER_POSITIVE_OUTCOME = 10.0\n",
     "DECISION_OPPORTUNITIES = [45, 60] # time in seconds at which the \n",
     "                                  # agent can decide to quit or wait\n",
-    "                                  # code is tailored to just 2 right now"
+    "                                  # code is tailored to just 2 right now\n",
+    "\n",
+    "LOW_RAM_MODE = True  # Recommended for Colab/local laptops\n",
+    "MAX_SEQ_LEN = 768 if LOW_RAM_MODE else 1024\n",
+    "TRAIN_BATCH_SIZE = 1 if LOW_RAM_MODE else 12\n",
+    "EVAL_BATCH_SIZE = 1 if LOW_RAM_MODE else 12\n",
+    "GRAD_ACCUM_STEPS = 16 if LOW_RAM_MODE else 1\n",
+    "GEN_BATCH_SIZE = 16 if LOW_RAM_MODE else 72\n"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "931864dc",
    "metadata": {},
    "source": [
     "## Load and process conversation data\n",
     "\n",
     "We load a dataset of synthetic conversations available\n",
     "in the `datasets` folder at [https://github.com/emaadmanzoor/stopping-agents/](https://github.com/emaadmanzoor/stopping-agents/). This example dataset is formatted in the PyAnnote diarized conversation format."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 2,
    "id": "99c5e357",
    "metadata": {},
    "outputs": [
     {
      "data": {
       "text/html": [
        "<div>\n",
        "<style scoped>\n",
        "    .dataframe tbody tr th:only-of-type {\n",
@@ -720,59 +729,78 @@
   },
   {
    "cell_type": "code",
    "execution_count": 7,
    "id": "db06d469",
    "metadata": {},
    "outputs": [
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
        "model_id": "7aa0faf14d8d43bcaea305837ff480f4",
        "version_major": 2,
        "version_minor": 0
       },
       "text/plain": [
        "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
       ]
      },
      "metadata": {},
      "output_type": "display_data"
     }
    ],
    "source": [
     "huggingface_hub.login(token=HF_TOKEN)\n",
     "\n",
-    "model_name = \"meta-llama/Llama-3.2-3B\" # base model, not instruction-tuned\n",
+    "# 1B is much easier to run on Colab/laptops than 3B.\n",
+    "model_name = \"meta-llama/Llama-3.2-1B\" if LOW_RAM_MODE else \"meta-llama/Llama-3.2-3B\"\n",
     "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
-    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")\n",
     "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
     "tokenizer.padding_side = 'left'\n",
     "\n",
+    "if torch.cuda.is_available() and LOW_RAM_MODE:\n",
+    "    bnb_config = transformers.BitsAndBytesConfig(\n",
+    "        load_in_4bit=True,\n",
+    "        bnb_4bit_quant_type=\"nf4\",\n",
+    "        bnb_4bit_compute_dtype=torch.float16,\n",
+    "    )\n",
+    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
+    "        model_name,\n",
+    "        quantization_config=bnb_config,\n",
+    "        device_map=\"auto\",\n",
+    "        low_cpu_mem_usage=True,\n",
+    "    )\n",
+    "else:\n",
+    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
+    "        model_name,\n",
+    "        torch_dtype=\"auto\",\n",
+    "        low_cpu_mem_usage=True,\n",
+    "    )\n",
+    "\n",
     "if len(tokenizer) > model.get_input_embeddings().weight.shape[0]:\n",
     "    print(\"WARNING: Resizing the embedding matrix to match the tokenizer vocab size.\")\n",
-    "    model.resize_token_embeddings(len(tokenizer))"
+    "    model.resize_token_embeddings(len(tokenizer))\n"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "240767e7",
    "metadata": {},
    "source": [
     "### Construct and tokenize fine-tuning datasets\n",
     "\n",
     "We perform manual masking, so the loss is only calculated for the generated actions."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 8,
    "id": "d8c94248",
    "metadata": {},
    "outputs": [
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
        "model_id": "ac592b7caaa94bc4b89d7fb9d868dacf",
        "version_major": 2,
        "version_minor": 0
       },
@@ -823,57 +851,60 @@
    "source": [
     "train_dataset = datasets.Dataset.from_dict(\n",
     "    {\"prompt\": [state for state in optimal_state_action_pairs[\"train\"][\"state\"].values], \n",
     "     \"completion\": [action.strip()\n",
     "                    for action in optimal_state_action_pairs[\"train\"][\"action\"].values]}).shuffle()\n",
     "    \n",
     "val_dataset = datasets.Dataset.from_dict(\n",
     "    {\"prompt\": [state for state in optimal_state_action_pairs[\"val\"][\"state\"].values],\n",
     "     \"completion\": [action.strip()\n",
     "                    for action in optimal_state_action_pairs[\"val\"][\"action\"].values]}).shuffle()\n",
     "\n",
     "def tokenize_fn(example, add_label):\n",
     "    # start with the BOS token if it exists\n",
     "    if tokenizer.bos_token is not None:\n",
     "        encoded_prompt = tokenizer.encode(tokenizer.bos_token +\n",
     "                                          example[\"prompt\"],              \n",
     "                                          add_special_tokens=False)\n",
     "    else:\n",
     "        encoded_prompt = tokenizer.encode(example[\"prompt\"], \n",
     "                                          add_special_tokens=False)\n",
     "\n",
     "    # add the label if needed for the training and validation datasets\n",
     "    if add_label:\n",
     "        encoded_label = tokenizer.encode(example[\"completion\"] + tokenizer.eos_token, \n",
     "                                         add_special_tokens=False)\n",
-    "        return {\"input_ids\": encoded_prompt + encoded_label,\n",
-    "                \"attention_mask\" : [1] * (len(encoded_prompt) + len(encoded_label)),\n",
-    "                \"labels\": [-100] * len(encoded_prompt) + encoded_label}\n",
+    "        input_ids = (encoded_prompt + encoded_label)[:MAX_SEQ_LEN]\n",
+    "        labels = ([-100] * len(encoded_prompt) + encoded_label)[:MAX_SEQ_LEN]\n",
+    "        return {\"input_ids\": input_ids,\n",
+    "                \"attention_mask\" : [1] * len(input_ids),\n",
+    "                \"labels\": labels}\n",
     "    else:\n",
-    "        return {\"input_ids\": encoded_prompt,\n",
-    "                \"attention_mask\": [1] * len(encoded_prompt),\n",
-    "                \"labels\": [-100] * len(encoded_prompt)}\n",
+    "        input_ids = encoded_prompt[:MAX_SEQ_LEN]\n",
+    "        return {\"input_ids\": input_ids,\n",
+    "                \"attention_mask\": [1] * len(input_ids),\n",
+    "                \"labels\": [-100] * len(input_ids)}\n",
     "\n",
     "train_dataset = train_dataset.map(tokenize_fn,\n",
     "                                  remove_columns=[\"prompt\", \"completion\"], \n",
     "                                  fn_kwargs={\"add_label\": True})\n",
     "val_dataset = val_dataset.map(tokenize_fn, \n",
     "                              remove_columns=[\"prompt\", \"completion\"], \n",
     "                              fn_kwargs={\"add_label\": True})\n",
     "\n",
     "print(\"Tokenization test:\")\n",
     "print(tokenizer.decode(train_dataset[0][\"input_ids\"]))\n",
     "print()\n",
     "print(\"Expected Label (action):\")\n",
     "print(tokenizer.decode([l for l in train_dataset[0][\"labels\"] if l!=-100]))"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "8f5717d6",
    "metadata": {},
    "source": [
     "### Fine-tune"
    ]
   },
   {
    "cell_type": "code",
@@ -956,93 +987,92 @@
     "NO_TOKEN_ID = tokenizer.encode(\"no\", add_special_tokens=False)[-1]\n",
     "\n",
     "def preprocess_logits_for_metrics(logits, labels):\n",
     "    \"\"\"\n",
     "    Original Trainer may have a memory leak. \n",
     "    This is a workaround to avoid storing too many tensors that are not needed.\n",
     "    \"\"\"\n",
     "    return logits[:, -3, :] # last non-padding token logits only, for causal LM\n",
     "\n",
     "def compute_metrics(eval_preds):\n",
     "    logits, labels = eval_preds\n",
     "    labels = [l[(l != -100) & (l != tokenizer.pad_token_id)][0] for l in labels]\n",
     "\n",
     "    logprobs = [torch.log_softmax(torch.from_numpy(s), dim=-1).numpy()\n",
     "                for s in logits]\n",
     "    labelprobs = [math.exp(logprob[label]) for logprob, label in zip(logprobs, labels)]\n",
     "\n",
     "    ytrue = [1 if label == YES_TOKEN_ID else 0 for label in labels]\n",
     "    ypred = [labelprob if label==YES_TOKEN_ID else 1.0 - labelprob\n",
     "             for labelprob, label in zip(labelprobs, labels)]\n",
     "    auc = roc_auc_score(ytrue, ypred)\n",
     "\n",
     "    return {\"auc\": auc}\n",
     "\n",
     "training_args = transformers.TrainingArguments(\n",
-    "    output_dir=\"./llama-3.2-3B/\",\n",
+    "    output_dir=\"./llama-3.2/\",\n",
     "    overwrite_output_dir=True,\n",
     "    remove_unused_columns=False,\n",
     "\n",
     "    save_strategy=\"best\",\n",
     "    logging_strategy=\"epoch\",\n",
     "    eval_strategy=\"epoch\",\n",
     "    save_total_limit=1,\n",
     "\n",
-    "    # on 1 H100 96GB: batch size of 12-16 for 3B works\n",
-    "    per_device_train_batch_size=12,\n",
-    "    per_device_eval_batch_size=12,\n",
-    "    gradient_accumulation_steps=1,\n",
+    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
+    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
+    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
     "\n",
     "    num_train_epochs=10,\n",
     "    learning_rate=1e-4,\n",
     "    optim=\"adamw_torch\",\n",
+    "    gradient_checkpointing=LOW_RAM_MODE,\n",
     "\n",
     "    load_best_model_at_end=True,\n",
     "    metric_for_best_model=\"auc\",\n",
     "    greater_is_better=True,\n",
     "\n",
     "    report_to=\"none\", # change to wandb if needed\n",
     "    save_safetensors=False, # needed to load saved models\n",
     "\n",
-    "    # change to suit hardware\n",
-    "    bf16=True, \n",
-    "    fp16=False,\n",
+    "    bf16=torch.cuda.is_available() and not LOW_RAM_MODE,\n",
+    "    fp16=torch.cuda.is_available() and LOW_RAM_MODE,\n",
     ")\n",
     "\n",
     "trainer = transformers.Trainer(\n",
     "    model=model,\n",
     "    args=training_args,\n",
     "    train_dataset=train_dataset.shuffle(),\n",
     "    eval_dataset=val_dataset,\n",
     "    processing_class=tokenizer,\n",
-    "    data_collator=transformers.data.DataCollatorForSeq2Seq(tokenizer),\n",
+    "    data_collator=transformers.data.DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8 if torch.cuda.is_available() else None),\n",
     "    compute_metrics=compute_metrics,\n",
     "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
     "    callbacks=[transformers.EarlyStoppingCallback(early_stopping_patience=1)]\n",
     ")\n",
     "\n",
-    "trainer.train(resume_from_checkpoint=False)"
+    "trainer.train(resume_from_checkpoint=False)\n"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "56c26914",
    "metadata": {},
    "source": [
     "### Evaluate"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "cdb95891",
    "metadata": {},
    "source": [
     "#### Get the validation and test responses for each state\n",
     "\n",
     "We get the agent's responses for the validation and test conversations. We use the validation responses for backward-induction threshold-tuning (our scalable alternative to grid search)."
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 18,
    "id": "88183bd6",
    "metadata": {},
@@ -1076,124 +1106,128 @@
      ]
     },
     {
      "data": {
       "application/vnd.jupyter.widget-view+json": {
        "model_id": "89a699eea10245818047fa37bed6ec25",
        "version_major": 2,
        "version_minor": 0
       },
       "text/plain": [
        "  0%|          | 0/1 [00:00<?, ?it/s]"
       ]
      },
      "metadata": {},
      "output_type": "display_data"
     }
    ],
    "source": [
     "val_prompts = list(optimal_state_action_pairs[\"val\"][\"state\"].values)\n",
     "test_prompts = list(optimal_state_action_pairs[\"test\"][\"state\"].values)\n",
     "\n",
     "print(\"Getting test responses...\")\n",
     "\n",
     "responses_test = []\n",
     "logprobs_test = []\n",
-    "batch_size = 72 # change to suit hardware\n",
+    "batch_size = GEN_BATCH_SIZE # lower for Colab/laptops\n",
     "\n",
     "for i in tqdm(range(0, len(test_prompts), batch_size)):\n",
     "    batch_prompts = test_prompts[i:i+batch_size]\n",
     "    batch = tokenizer(batch_prompts, \n",
     "                      return_tensors=\"pt\", \n",
     "                      padding=True, \n",
     "                      add_special_tokens=True,\n",
     "                      truncation=True).to(\"cuda\")\n",
     "\n",
     "    with torch.no_grad():\n",
     "        outputs = trainer.model.generate(\n",
     "            **batch, \n",
     "            max_new_tokens=2, \n",
     "            do_sample=False,\n",
     "            pad_token_id=tokenizer.eos_token_id,\n",
     "            temperature=None, top_p=None, top_k=None,\n",
     "            return_dict_in_generate=True, output_scores=True\n",
     "            # greedy decoding: so output_scores = output_logits\n",
     "        )\n",
     "\n",
     "    seqs = outputs.sequences\n",
     "    prompt_len = batch['input_ids'].shape[1]   # Length of the input prompts\n",
     "\n",
     "    # Slice to get only the generated new tokens\n",
     "    generated_tokens = seqs[:, prompt_len:]\n",
     "    decoded_outputs = tokenizer.batch_decode(generated_tokens,\n",
     "                                             skip_special_tokens=True)\n",
     "    decoded_outputs = [d.strip().lower() for d in decoded_outputs]\n",
     "\n",
     "    responses_test.extend(decoded_outputs)\n",
     "\n",
     "    scores = outputs.scores\n",
     "    logprobs = [torch.log_softmax(s, dim=-1) for s in scores]\n",
     "    logprobs = logprobs[0][torch.arange(logprobs[0].size(0)), seqs[:, -2].view(-1)]\n",
     "    logprobs_test.extend(logprobs.cpu().numpy())\n",
     "\n",
     "print(\"Getting validation responses...\")\n",
     "\n",
     "responses_val = []\n",
     "logprobs_val = []\n",
-    "batch_size = 72\n",
+    "batch_size = GEN_BATCH_SIZE\n",
     "\n",
     "for i in tqdm(range(0, len(val_prompts), batch_size)):\n",
     "    batch_prompts = val_prompts[i:i+batch_size]\n",
     "    batch = tokenizer(batch_prompts, \n",
     "                      return_tensors=\"pt\", \n",
     "                      padding=True, \n",
     "                      truncation=True).to(\"cuda\")\n",
     "\n",
     "    with torch.no_grad():\n",
     "        outputs = trainer.model.generate(\n",
     "            **batch, \n",
     "            max_new_tokens=2, \n",
     "            do_sample=False,\n",
     "            pad_token_id=tokenizer.eos_token_id,\n",
     "            temperature=None, top_p=None, top_k=None,\n",
     "            return_dict_in_generate=True, output_scores=True\n",
     "        )\n",
     "\n",
     "    seqs = outputs.sequences\n",
     "    prompt_len = batch['input_ids'].shape[1]   # Length of the input prompts\n",
     "\n",
     "    # Slice to get only the generated new tokens\n",
     "    generated_tokens = seqs[:, prompt_len:]\n",
     "    decoded_outputs = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
     "    decoded_outputs = [d.strip().lower() for d in decoded_outputs]\n",
     "\n",
     "    responses_val.extend(decoded_outputs)\n",
     "\n",
     "    scores = outputs.scores\n",
     "    logprobs = [torch.log_softmax(s, dim=-1) for s in scores]\n",
     "    logprobs = logprobs[0][torch.arange(logprobs[0].size(0)), seqs[:, -2].view(-1)]\n",
-    "    logprobs_val.extend(logprobs.cpu().numpy())"
+    "    logprobs_val.extend(logprobs.cpu().numpy())\n",
+    "# Free memory before threshold tuning\n",
+    "gc.collect()\n",
+    "if torch.cuda.is_available():\n",
+    "    torch.cuda.empty_cache()\n"
    ]
   },
   {
    "cell_type": "markdown",
    "id": "86c7a682",
    "metadata": {},
    "source": [
     "#### Store validation and test responses"
    ]
   },
   {
    "cell_type": "code",
    "execution_count": 20,
    "id": "772d7131",
    "metadata": {},
    "outputs": [
     {
      "name": "stdout",
      "output_type": "stream",
      "text": [
       "Val 45 ROC-AUC: 1.00\n",
       "Test 45 ROC-AUC: 0.99\n",
       "Val 60 ROC-AUC: 1.00\n",
       "Test 60 ROC-AUC: 0.98\n"
      ]
